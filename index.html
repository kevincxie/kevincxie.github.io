---
layout: default
pagination: 
  enabled: true
---
<!-- <div class="home"> -->

<div class="wrapper">
<div class="page">

  <header class="post-header">
    <h1 class="post-title">Kevin Xie</h1>
  </header>


  <article class="post-content">
    <!-- {{ content }} -->

    I am a PhD student in the Department of Computer Science at the University of Toronto and the Vector Institute, gratefully co-supervised by Florian Shkurti and Sanja Fidler.
    I am also a Research Scientist on the Toronto AI Lab at Nvidia.
    Before that I did my undergrad in Engineering Physics at UBC, where I had the great pleasure of working with Michiel van de Panne and his group.
    <br>
    My research interests surround learning-based methods for simulation and control and include generative modelling, model-based reinforcement learning and character control.
    <br>
    Email: <img src='assets/email.png' style="display:inline;height:1em;width:150px">
  </article>
  <!-- {% include page_divider.html %} -->

  {% include page_divider.html %}
  <h2> Publications</h2>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
        <td style="padding:6px;width:33%;vertical-align:middle">
          <!-- <div style = "height:2.5em"> -->
            <!-- <img src='assets/PLAiD_teaser.png' class="heightSet"> -->
            <img class="paper-pic" src='assets/phys_pe.png'>
            <!-- </div> -->
        </td>
        <td style="padding:6px;width:66%;vertical-align:middle">
          <strong>Physics-based Human Motion Estimation and Synthesis from Videos</strong>
          <br>
          <strong>Kevin Xie</strong>,
          <a>Tingwu Wang</a>,
          <a>Umar Iqbal</a>,
          <a>Yunrong Guo</a>,
          <a>Sanja Fidler</a>
          <a>Florian Shkurti</a>
          <!-- <a href="https://arxiv.org/abs/2010.08888"> -->
          <!-- </a> -->
          <br>
            Video pose estimation quality can be improved greatly by enforcing physics consistency.
          <br>
          <em>ICCV</em>, 2021
          <!-- <a href="https://arxiv.org/abs/2010.08888">arXiv</a> -->
          <a href="https://arxiv.org/abs/2109.09913">paper</a>
          <a href="https://nv-tlabs.github.io/physics-pose-estimation-project-page/">website</a>
          <p></p>
        </td>
    </tr>
    <tr>
        <td style="padding:6px;width:33%;vertical-align:middle">
          <!-- <div style = "height:2.5em"> -->
            <!-- <img src='assets/PLAiD_teaser.png' class="heightSet"> -->
            <img class="paper-pic" src='assets/lsp.png'>
            <!-- </div> -->
        </td>
        <td style="padding:6px;width:66%;vertical-align:middle">
          <strong>Skill Transfer via Partially Amortized Hierarchical Planning</strong>
          <br>
          <strong>Kevin Xie*</strong>,
          <a>Homanga Bharadhwaj*</a>,
          <a>Danijar Hafner</a>,
          <a>Animesh Garg</a>,
          <a>Florian Shkurti</a>
          <!-- <a href="https://arxiv.org/abs/2010.08888"> -->
          <!-- </a> -->
          <br>
            Learning temporally extended skills for planning with a world model improves MBRL training and transfer.
          <br>
          <em>ICLR</em>, 2021
          <!-- <a href="https://arxiv.org/abs/2010.08888">arXiv</a> -->
          <a href="https://arxiv.org/pdf/2011.13897.pdf">paper</a>
          <a href="https://sites.google.com/view/latent-skill-planning/home">website</a>
          <p></p>
        </td>
    </tr>

    <tr>
        <td style="padding:6px;width:33%;vertical-align:middle">
          <!-- <div style = "height:2.5em"> -->
            <!-- <img src='assets/PLAiD_teaser.png' class="heightSet"> -->
            <img  class="paper-pic" src='assets/hclr.png'>
            <!-- </div> -->
        </td>
        <td style="padding:6px;width:66%;vertical-align:middle">
          <strong>Continual Model-Based Reinforcement Learning with Hypernetworks</strong>
          <br>
          <a>Philip Huang</a>,
          <strong>Kevin Xie</strong>,
          <a>Homanga Bharadhwaj</a>,
          <a>Florian Shkurti</a>
          <!-- <a href="https://arxiv.org/abs/2010.08888"> -->
          <!-- </a> -->
          <br>
            Task-conditioned hypernetworks continually adapt to change in environment dynamics with a limited replay buffer.
          <br>
          <em>Deep RL Workshop (NeurIPS)</em>, 2020
          <!-- <a href="https://arxiv.org/abs/2010.08888">arXiv</a> -->
          <a href="https://arxiv.org/pdf/2009.11997.pdf">paper</a>
          <p></p>
        </td>
    </tr>

    <tr>
        <td style="padding:6px;width:33%;vertical-align:middle">
          <!-- <div style = "height:2.5em"> -->
            <!-- <img src='assets/PLAiD_teaser.png' class="heightSet"> -->
            <img  class="paper-pic" src='assets/gradcem.png'>
            <!-- </div> -->
        </td>
        <td style="padding:6px;width:66%;vertical-align:middle">
          <strong>Model-Predictive Planning via Cross-Entropy and Gradient-Based Optimization</strong>
          <br>
          <a>Homanga Bharadhwaj*</a>,
          <strong>Kevin Xie*</strong>,
          <a>Florian Shkurti</a>,
          <!-- <a href="https://arxiv.org/abs/2010.08888"> -->
          <!-- </a> -->
          <br>
            Updating the top action sequences identified by CEM through a few gradient steps helps improve sample efficiency and performance of planning in Model-based RL.
          <br>
          <em>L4DC</em>, 2020
          <a href="http://proceedings.mlr.press/v120/bharadhwaj20a/bharadhwaj20a.pdf">paper</a>
          <p></p>
        </td>
    </tr>

    <tr>
        <td style="padding:6px;width:33%;vertical-align:middle">
          <!-- <div style = "height:2.5em"> -->
            <!-- <img src='assets/PLAiD_teaser.png' class="heightSet"> -->
            <img  class="paper-pic" src='assets/PLAiD_teaser.png'>
            <!-- </div> -->
        </td>
        <td style="padding:6px;width:66%;vertical-align:middle">
          <strong>Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control</strong>
          <br>
          <a>Glen Berseth*</a>,
          <strong>Kevin Xie*</strong>,
          <a>Paul Cernek</a>,
          <a>Michiel van de Panne</a>
          <!-- <a href="https://arxiv.org/abs/2010.08888"> -->
          <!-- </a> -->
          <br>
            Progressive learning and integration via distillation (PLAID) allows a single policy to quickly acquire new locomotion skills.
          <br>
          <em>ICLR</em>, 2018 
          <a href="https://arxiv.org/pdf/1802.04765.pdf">arXiv</a>
          <p></p>
        </td>
    </tr>
  </tbody></table>

</div>
</div>

<!-- </div> -->
